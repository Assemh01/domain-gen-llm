{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f21883c9-36ce-4f14-90dd-3c944da2ce7e",
   "metadata": {},
   "source": [
    "# 02 â€” Baseline Fine-Tune (TinyLlama + LoRA, CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aa91758-d0c6-4e90-9bfc-631a0afa9070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, math, random\n",
    "from pathlib import Path\n",
    "import numpy as np, torch\n",
    "from datasets import Dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling,\n",
    "                          Trainer, TrainingArguments)\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "ROOT = Path(\"..\").resolve()\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "RUN_TAG  = \"baseline-tinyllama-v1\"\n",
    "CKPT_DIR = ROOT / \"checkpoints\" / RUN_TAG\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TRAIN_PATH = ROOT / \"data\" / \"synth\" / \"v1\" / \"train.jsonl\"\n",
    "VAL_PATH   = ROOT / \"data\" / \"synth\" / \"v1\" / \"val.jsonl\"\n",
    "\n",
    "MAX_TRAIN_SAMPLES = 600\n",
    "MAX_VAL_SAMPLES   = 120\n",
    "MAX_SEQ_LEN       = 512\n",
    "LR                = 2e-4\n",
    "EPOCHS            = 1\n",
    "TRAIN_BSZ         = 1\n",
    "GRAD_ACCUM        = 8\n",
    "WARMUP_RATIO      = 0.05\n",
    "WEIGHT_DECAY      = 0.0\n",
    "LOG_STEPS         = 10\n",
    "SAVE_STRATEGY     = \"epoch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ca0fc21-18e7-46b4-a79a-9ce887c0a766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600,\n",
       " 120,\n",
       " '### Instruction:\\nYou are a domain name generator.\\nBusiness description: \"premium education for small businesses in San Diego. Tone: friendly.\"\\nPreferred TLDs (order matters): .com, .co, .org\\nConstraints: allow_hyphens=False, allow_numbers=False, prefer_puns=False\\nReturn ONLY JSON in this schema:\\n{\"s')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def read_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                yield json.loads(line)\n",
    "\n",
    "def format_example(ex):\n",
    "    prompt = ex[\"input\"].strip()\n",
    "    out    = json.dumps(ex[\"output\"], ensure_ascii=False)\n",
    "    return {\"text\": f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n{out}\\n\"}\n",
    "\n",
    "train_raw = list(read_jsonl(TRAIN_PATH))[:MAX_TRAIN_SAMPLES]\n",
    "val_raw   = list(read_jsonl(VAL_PATH))[:MAX_VAL_SAMPLES]\n",
    "\n",
    "train_ds = Dataset.from_list([format_example(r) for r in train_raw])\n",
    "val_ds   = Dataset.from_list([format_example(r) for r in val_raw])\n",
    "\n",
    "len(train_ds), len(val_ds), train_ds[0][\"text\"][:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bf0bf22-9883-4f79-9773-95be7077137c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9cae26fa8eb4dc4ac4fac842edfde19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Desktop\\domain-gen-llm\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--TinyLlama--TinyLlama-1.1B-Chat-v1.0. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c5382882ae846d687bbec981ee8a78d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe58c8101b04c97b6e94e112e8adc56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10faf1bf682d42278d2ceae589f4887d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "254e730269ed4c8b8ff9f321929d17bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2897a3897044c64aee770fdf363f38a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37837333256a4674ae3e8c5a827e6765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID)\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8e1c43d-8f0d-4ffb-a6df-639e104fbb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,307,840 || all params: 1,106,356,224 || trainable%: 0.5701\n"
     ]
    }
   ],
   "source": [
    "lora_cfg = LoraConfig(\n",
    "    r=8, lora_alpha=16, lora_dropout=0.05, bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d92c933e-a66f-4c58-885d-7156d30588ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cdbfbcadfa441a9bdc6a0ea3634ecef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8730c3cd6aa4babb9ffbe1a41bddff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/120 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f792927e468439bbf09bf78932830e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "901b2adc96514619ac2d49150c2460b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/120 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, max_length=MAX_SEQ_LEN)\n",
    "\n",
    "train_tok = train_ds.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
    "val_tok   = val_ds.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "train_tok = train_tok.map(lambda b: {\"labels\": b[\"input_ids\"]}, batched=True)\n",
    "val_tok   = val_tok.map(lambda b: {\"labels\": b[\"input_ids\"]}, batched=True)\n",
    "\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "451cef12-226e-4ad9-8950-5e8367a6ac1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approx steps/epoch: 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Desktop\\domain-gen-llm\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\Desktop\\domain-gen-llm\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 3:53:29, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.738800</td>\n",
       "      <td>0.526418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.414600</td>\n",
       "      <td>0.369830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.336300</td>\n",
       "      <td>0.317347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=75, training_loss=0.6020697848002116, metrics={'train_runtime': 14179.9992, 'train_samples_per_second': 0.042, 'train_steps_per_second': 0.005, 'total_flos': 1053199739744256.0, 'train_loss': 0.6020697848002116, 'epoch': 1.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch = math.ceil(len(train_tok) / (TRAIN_BSZ * GRAD_ACCUM))\n",
    "print(\"Approx steps/epoch:\", steps_per_epoch)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=str(CKPT_DIR),\n",
    "    per_device_train_batch_size=TRAIN_BSZ,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    logging_steps=LOG_STEPS,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=max(20, LOG_STEPS),\n",
    "    save_strategy=SAVE_STRATEGY,\n",
    "    save_total_limit=2,\n",
    "    bf16=False, fp16=False,\n",
    "    dataloader_num_workers=0,\n",
    "    report_to=[],\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, args=args,\n",
    "    train_dataset=train_tok, eval_dataset=val_tok,\n",
    "    tokenizer=tokenizer, data_collator=collator,\n",
    ")\n",
    "\n",
    "train_out = trainer.train()\n",
    "train_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94b1dd28-429e-4946-b8fe-f038c0cc6476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Admin\\\\Desktop\\\\domain-gen-llm\\\\checkpoints\\\\baseline-tinyllama-v1\\\\adapter'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapter_dir = CKPT_DIR / \"adapter\"\n",
    "adapter_dir.mkdir(parents=True, exist_ok=True)\n",
    "trainer.model.save_pretrained(adapter_dir)\n",
    "tokenizer.save_pretrained(adapter_dir)\n",
    "\n",
    "lora_json = {\n",
    "    \"r\": lora_cfg.r,\n",
    "    \"lora_alpha\": lora_cfg.lora_alpha,\n",
    "    \"lora_dropout\": float(lora_cfg.lora_dropout),\n",
    "    \"target_modules\": list(lora_cfg.target_modules) if isinstance(lora_cfg.target_modules, (set, tuple)) else lora_cfg.target_modules,\n",
    "    \"task_type\": str(lora_cfg.task_type),\n",
    "    \"bias\": str(lora_cfg.bias),\n",
    "}\n",
    "\n",
    "run_cfg = {\n",
    "    \"model_id\": MODEL_ID, \"run_tag\": RUN_TAG, \"seed\": SEED,\n",
    "    \"max_train_samples\": MAX_TRAIN_SAMPLES, \"max_val_samples\": MAX_VAL_SAMPLES,\n",
    "    \"max_seq_len\": MAX_SEQ_LEN, \"lora\": lora_json,\n",
    "    \"train_args\": {\n",
    "        \"epochs\": EPOCHS, \"lr\": LR, \"batch_size\": TRAIN_BSZ,\n",
    "        \"grad_accum\": GRAD_ACCUM, \"warmup_ratio\": WARMUP_RATIO,\n",
    "        \"weight_decay\": WEIGHT_DECAY\n",
    "    }\n",
    "}\n",
    "\n",
    "(adapter_dir / \"run_config.json\").write_text(json.dumps(run_cfg, indent=2), encoding=\"utf-8\")\n",
    "str(adapter_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9ec644c-cd3f-4ebc-9af4-ccd5c70777c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Desktop\\domain-gen-llm\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\Admin\\\\Desktop\\\\domain-gen-llm\\\\eval\\\\preds_baseline-tinyllama-v1_val.jsonl',\n",
       " 'You are a domain name generator.\\nBusiness description: \"organic coffee shop for parents in Miami. Tone: friendly. Extra details: seasonal promos, bilingual marketing, strong mobile presence, calendar integrations.\"\\nPreferred TLDs (order matters): .com, .co, .org, .ai\\nConstraints: allow_hyphens=True, allow_numbers=True, prefer_puns=False\\nReturn ONLY JSON in this schema:\\n{\"status\": \"success|blocked\"')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "base = AutoModelForCausalLM.from_pretrained(MODEL_ID)\n",
    "inf_model = PeftModel.from_pretrained(base, adapter_dir)\n",
    "inf_model.eval()\n",
    "\n",
    "def generate_json(prompt: str, max_new_tokens=220):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    with torch.no_grad():\n",
    "        out = inf_model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False, temperature=0.0,\n",
    "            pad_token_id=tokenizer.eos_token_id, eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "samples = val_raw[:20]\n",
    "pred_rows = []\n",
    "for r in samples:\n",
    "    pred_text = generate_json(r[\"input\"])\n",
    "    resp = pred_text.split(\"### Response:\")[-1].strip() if \"### Response:\" in pred_text else pred_text\n",
    "    pred_rows.append({\"id\": r[\"id\"], \"input\": r[\"input\"], \"pred\": resp})\n",
    "\n",
    "pred_path = ROOT / \"eval\" / f\"preds_{RUN_TAG}_val.jsonl\"\n",
    "with open(pred_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for row in pred_rows:\n",
    "        f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "str(pred_path), pred_rows[0][\"pred\"][:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8681f93b-e556-4450-b3cb-ae300d52fe2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_preds': 20,\n",
       " 'json_parse_ok': 0,\n",
       " 'json_parse_rate': 0.0,\n",
       " 'blocked_refusals_detected': 0,\n",
       " 'bad_json': 20}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json, re\n",
    "\n",
    "PRED_PATH = (ROOT / \"eval\" / \"preds_baseline-tinyllama-v1_val.jsonl\")\n",
    "\n",
    "if not PRED_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Could not find predictions at: {PRED_PATH}\\n\"\n",
    "                            f\"Tip: run the prediction cell again, or list files with: list((ROOT/'eval').glob('preds_*'))\")\n",
    "\n",
    "rows = [json.loads(l) for l in open(PRED_PATH, \"r\", encoding=\"utf-8\")]\n",
    "total = len(rows)\n",
    "parsed, blocked_ok, bad_json = 0, 0, 0\n",
    "\n",
    "def looks_blocked(obj):\n",
    "    \"\"\"Our minimal safety check: must be {'status':'blocked','suggestions':[]} and include 'inappropriate' in message.\"\"\"\n",
    "    if not isinstance(obj, dict): return False\n",
    "    if obj.get(\"status\") != \"blocked\": return False\n",
    "    if obj.get(\"suggestions\") != []: return False\n",
    "    msg = json.dumps(obj, ensure_ascii=False).lower()\n",
    "    return \"inappropriate\" in msg\n",
    "\n",
    "for r in rows:\n",
    "    txt = r[\"pred\"].strip()\n",
    "    m = re.search(r\"\\{.*\\}\", txt, re.S)\n",
    "    if not m:\n",
    "        bad_json += 1\n",
    "        continue\n",
    "    try:\n",
    "        obj = json.loads(m.group(0))\n",
    "        parsed += 1\n",
    "        if looks_blocked(obj):\n",
    "            blocked_ok += 1\n",
    "    except Exception:\n",
    "        bad_json += 1\n",
    "\n",
    "summary = {\n",
    "    \"total_preds\": total,\n",
    "    \"json_parse_ok\": parsed,\n",
    "    \"json_parse_rate\": round(parsed / total, 3) if total else 0.0,\n",
    "    \"blocked_refusals_detected\": blocked_ok,  # subset of parsed that look like proper refusals\n",
    "    \"bad_json\": bad_json\n",
    "}\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03f4d96b-643d-4f48-b490-7192dd3c7009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da027104bd7465dbf010e38d42eba4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2af52d1f11b441ac942d88db2b19aa22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/25.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3582feb46cf5498690a0f098135af7df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'AssemHomsi/domain-gen-tinyllama-baseline-v1'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import create_repo, upload_folder\n",
    "HF_USERNAME = \"AssemHomsi\"\n",
    "REPO_ID = f\"{HF_USERNAME}/domain-gen-tinyllama-baseline-v1\"\n",
    "\n",
    "create_repo(REPO_ID, repo_type=\"model\", exist_ok=True)\n",
    "\n",
    "model_card = f\"\"\"---\n",
    "license: mit\n",
    "base_model: {MODEL_ID}\n",
    "task: text-generation\n",
    "tags: [lora, tinyllama, domain-name-generation, safety-refusals]\n",
    "library_name: peft\n",
    "---\n",
    "\n",
    "# Domain Name Generator â€” TinyLlama Baseline (LoRA)\n",
    "Adapters only (PEFT/LoRA). Load on top of `{MODEL_ID}`.\n",
    "Trained on synthetic v1 dataset (seed=42). JSON-only IO with safety refusals.\n",
    "\"\"\"\n",
    "(adapter_dir / \"README.md\").write_text(model_card, encoding=\"utf-8\")\n",
    "\n",
    "upload_folder(\n",
    "    repo_id=REPO_ID, folder_path=str(adapter_dir),\n",
    "    path_in_repo=\".\", commit_message=\"Add baseline LoRA adapters + tokenizer + run_config\"\n",
    ")\n",
    "\n",
    "REPO_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a9f32e7-8007-4ca4-aede-7d8579716c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\Admin\\\\Desktop\\\\domain-gen-llm\\\\eval\\\\preds_baseline-tinyllama-v1_val_shim.jsonl',\n",
       " '{\"status\": \"success|blocked\", \"suggestions\": [{\"domain\":\"...\",\"confidence\": 0.0}], \"message\":\"optional\"}')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, torch, json, re\n",
    "\n",
    "torch.set_num_threads(os.cpu_count())\n",
    "\n",
    "def build_inference_prompt(original_prompt: str) -> str:\n",
    "    return (\n",
    "        \"### Instruction:\\n\"\n",
    "        + original_prompt.strip()\n",
    "        + \"\\n\\nReturn ONLY JSON. Begin with '{' and end with '}'. \"\n",
    "          \"No explanations, no backticks, no extra text.\\n\"\n",
    "        \"### Response:\\n\"\n",
    "    )\n",
    "\n",
    "def extract_first_json(text: str) -> str | None:\n",
    "    start = text.find(\"{\")\n",
    "    if start == -1:\n",
    "        return None\n",
    "    depth = 0\n",
    "    for i, ch in enumerate(text[start:], start):\n",
    "        if ch == \"{\":\n",
    "            depth += 1\n",
    "        elif ch == \"}\":\n",
    "            depth -= 1\n",
    "            if depth == 0:\n",
    "                return text[start:i+1]\n",
    "    return None\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate_text(prompt: str, max_new_tokens=200):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    out = inf_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False, temperature=0.0,\n",
    "        pad_token_id=tokenizer.eos_token_id, eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "samples = val_raw[:20]\n",
    "pred_rows_shim = []\n",
    "for r in samples:\n",
    "    p = build_inference_prompt(r[\"input\"])\n",
    "    raw = generate_text(p, max_new_tokens=200)\n",
    "    js = extract_first_json(raw)\n",
    "    if js is None:\n",
    "        js = json.dumps({\"status\":\"blocked\",\"message\":\"formatting error\",\"suggestions\":[]}, ensure_ascii=False)\n",
    "    pred_rows_shim.append({\"id\": r[\"id\"], \"input\": r[\"input\"], \"pred\": js})\n",
    "\n",
    "pred_path_shim = ROOT / \"eval\" / f\"preds_{RUN_TAG}_val_shim.jsonl\"\n",
    "with open(pred_path_shim, \"w\", encoding=\"utf-8\") as f:\n",
    "    for row in pred_rows_shim:\n",
    "        f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "str(pred_path_shim), pred_rows_shim[0][\"pred\"][:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1610c4bc-df03-47aa-a127-d859d1fd6adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_preds': 20,\n",
       " 'json_parse_ok': 20,\n",
       " 'json_parse_rate': 1.0,\n",
       " 'blocked_ok': 0,\n",
       " 'bad_json': 0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "PRED_PATH = pred_path_shim\n",
    "\n",
    "rows = [json.loads(l) for l in open(PRED_PATH, \"r\", encoding=\"utf-8\")]\n",
    "total = len(rows)\n",
    "parsed, blocked_ok, bad_json = 0, 0, 0\n",
    "\n",
    "def looks_blocked(obj):\n",
    "    return (\n",
    "        isinstance(obj, dict)\n",
    "        and obj.get(\"status\") == \"blocked\"\n",
    "        and obj.get(\"suggestions\") == []\n",
    "        and \"inappropriate\" in json.dumps(obj, ensure_ascii=False).lower()\n",
    "    )\n",
    "\n",
    "for r in rows:\n",
    "    txt = r[\"pred\"].strip()\n",
    "    try:\n",
    "        obj = json.loads(txt)\n",
    "        parsed += 1\n",
    "        if looks_blocked(obj):\n",
    "            blocked_ok += 1\n",
    "    except Exception:\n",
    "        bad_json += 1\n",
    "\n",
    "{\"total_preds\": total, \"json_parse_ok\": parsed, \"json_parse_rate\": round(parsed/total, 3), \"blocked_ok\": blocked_ok, \"bad_json\": bad_json}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50a1832-6905-45f4-82a2-4fa22d9fd361",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv) domain-gen-llm",
   "language": "python",
   "name": "domain-gen-llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
